{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864bd90f-e912-407b-af95-128d310597e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81a7276-476c-47ee-aa2f-d3a8f761b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/HeadHunter_train.csv', index_col=0)\n",
    "test_df = pd.read_csv('data/HeadHunter_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c975b5-9f9a-4c29-98aa-b93e4c3a435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_cities = train_df.append(test_df).city.value_counts()[:50].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf987e8c-cd09-4a9c-bc0b-e41643814144",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['city_cat'] = train_df.position.apply(lambda text: text if text in popular_cities else 'Другое')\n",
    "test_df['city_cat'] = test_df.position.apply(lambda text: text if text in popular_cities else 'Другое')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c8014ec-8b06-47e7-9b57-fb9b52e20f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = (\n",
    "    train_df['positive']\n",
    "    .append(train_df['negative'])\n",
    "    .append(test_df['positive'])\n",
    "    .append(test_df['negative'])\n",
    "    .fillna('None')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c1b3d33-e6ee-41d0-8eb3-2e2a484eb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalizer import normalize_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6b6aaf6-05e4-4561-bed9-244691d9abc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 203054/203054 [00:15<00:00, 13529.43it/s]\n",
      "100%|███████████████████████████████████| 92678/92678 [00:21<00:00, 4318.04it/s]\n",
      "100%|███████████████████████████████| 203054/203054 [00:01<00:00, 165223.99it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = normalize_sentences(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22e3e3e8-d627-4e12-9b1f-5c87e585c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\" \".join([word for word in text if len(word) > 3]) for text in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61abade5-f960-4cbb-b8b1-548dc0b2b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['normal_positive'] = sentences[:train_df.shape[0]]\n",
    "train_df['normal_negative'] = sentences[train_df.shape[0]:2*train_df.shape[0]]\n",
    "test_df['normal_positive'] = sentences[2*train_df.shape[0]:2*train_df.shape[0]+test_df.shape[0]]\n",
    "test_df['normal_negative'] = sentences[2*train_df.shape[0]+test_df.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a76144b5-66d3-4aca-9b3a-8fe42bb5212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target'] = train_df['target'].str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc214b15-ccc0-4ac0-a8f7-f4f742c248ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.explode('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53ff52a0-b3c4-4b56-b042-de0e39c2aeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53753, 14)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aea43a2-8d02-4f89-a35e-1751a944607c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 104404/104404 [00:01<00:00, 55191.00it/s]\n",
      "100%|███████████████████████████████████| 10096/10096 [00:02<00:00, 4139.59it/s]\n",
      "100%|███████████████████████████████| 104404/104404 [00:00<00:00, 400951.25it/s]\n"
     ]
    }
   ],
   "source": [
    "all_positions = train_df.append(test_df)['position']\n",
    "clear_positions = normalize_sentences(all_positions.fillna('None'))\n",
    "clear_positions = [\" \".join(text) for text in clear_positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06567274-d420-4e56-9c7d-d530e86e4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['normal_position'] = clear_positions[:train_df.shape[0]]\n",
    "test_df['normal_position'] = clear_positions[train_df.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38b6ebcb-4e43-4c04-894e-83fb53d1a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['positive', 'negative', 'position'], axis=1).to_csv('data/clean_train.csv', index=False)\n",
    "test_df.drop(['positive', 'negative', 'position'], axis=1).to_csv('data/clean_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea6c50a0-ece6-444c-86fa-85072ddb8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/clean_train.csv')\n",
    "test_df = pd.read_csv('data/clean_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16792f7a-b8a6-41c9-ae76-313545ef1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['positive_len'] = train_df.normal_positive.str.len()\n",
    "train_df['negative_len'] = train_df.normal_negative.str.len()\n",
    "test_df['positive_len'] = test_df.normal_positive.str.len()\n",
    "test_df['negative_len'] = test_df.normal_negative.str.len()\n",
    "train_df['position_len'] = train_df.normal_position.str.len()\n",
    "test_df['position_len'] = test_df.normal_position.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "900bacc3-6a4d-4485-89e2-454505172623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15b0395d-3490-4fa0-b485-3773ad05bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectColumnsTransformer():\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        cpy_df = X[self.columns].copy()\n",
    "        return cpy_df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cb96042c-6678-4962-b794-85bacc3a6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "exception = ['review_id', target]\n",
    "real_columns = [col for col in train_df.columns if col not in exception and not isinstance(train_df.iloc[0][col], str)]\n",
    "cat_columns = ['city']\n",
    "text_columns = ['normal_positive', 'normal_negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "37e2b562-d63d-45d2-ad5a-bf70d27ee990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasSimpleImputer(SimpleImputer):\n",
    "    \"\"\"A wrapper around `SimpleImputer` to return data frames with columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = X.columns\n",
    "        return super().fit(X, y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(super().transform(X), columns=self.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "622a9181-b72e-4f3a-a277-826470fb68df",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "        (\"poly\",  PolynomialFeatures(2)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "        (\"encoder\", OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "def get_text_transformer(transformer):\n",
    "    return Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", PandasSimpleImputer(strategy=\"constant\", fill_value='None')),\n",
    "            ('transformer', ColumnTransformer([\n",
    "                ('position', TfidfVectorizer(max_features=1000, ngram_range=(1, 2)), 'normal_position'),\n",
    "                *[\n",
    "                (f\"text_{col}\", transformer, col)\n",
    "                for col in text_columns\n",
    "            ]]))\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8c400cea-2a55-4db6-bbf7-72c8365794cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4c694a15-6e5a-43e8-8d46-70555ba02cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Support Vector classifier\n",
    "clf_svc = SVC(C = 50, degree = 1, gamma = \"auto\", kernel = \"rbf\", probability = True)\n",
    "\n",
    "# Initializing Multi-layer perceptron  classifier\n",
    "clf_mlp = MLPClassifier(activation = \"relu\", alpha = 0.1, hidden_layer_sizes = (10,10,10),\n",
    "                            learning_rate = \"constant\", max_iter = 2000, random_state = 1000)\n",
    "\n",
    "# Initializing Random Forest classifier\n",
    "clf_rfc = RandomForestClassifier(n_estimators = 500, criterion = \"gini\", max_depth = 10,\n",
    "                                     max_features = \"auto\", min_samples_leaf = 0.005,\n",
    "                                     min_samples_split = 0.005, n_jobs = -1, random_state = 1000)\n",
    "\n",
    "classifiers = [('svc', clf_svc),\n",
    "               ('mlp', clf_mlp),                             \n",
    "               ('rfc', clf_rfc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "65100f11-e33d-4d39-bf29-26a4d06b0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_model = StackingClassifier(\n",
    "    estimators=classifiers, \n",
    "    final_estimator=LogisticRegression(),\n",
    "    stack_method='auto',\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "85cf3bca-f292-4ea9-abf5-17e8e1096234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_with_text_transformer(text_transformer, clf_model):\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, real_columns),\n",
    "            (\"cat\", categorical_transformer, cat_columns),\n",
    "            (\"text\", text_transformer, text_columns + ['normal_position']),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    pipe = Pipeline(steps=[\n",
    "        ('transformer', preprocessor),\n",
    "        ('model', clf_model)\n",
    "    ])\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7e2db2c8-44b4-4eb3-9bcf-033b329d8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df.drop(target, axis=1), \n",
    "    train_df[target], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "is_high_target = y_train.isin([0, 8, 1, 3, 6, 7, 5])\n",
    "X_train = X_train[is_high_target]\n",
    "y_train = y_train[is_high_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "91ab791e-5cf2-433a-be3d-0e7c93c4b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_transformer = get_text_transformer(TfidfVectorizer(max_features=1000))\n",
    "tfidf_pipe = pipe_with_text_transformer(tfidf_text_transformer, stacked_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "94af1404-a522-416b-9f55-7eaeb800389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.46 s, sys: 1.18 s, total: 8.64 s\n",
      "Wall time: 46min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michil/.virtualenvs/base/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transformer',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('poly',\n",
       "                                                                   PolynomialFeatures()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['salary_rating',\n",
       "                                                   'team_rating',\n",
       "                                                   'managment_rating',\n",
       "                                                   'career_rating',\n",
       "                                                   'workplace_rating',\n",
       "                                                   'rest_recovery_rating',\n",
       "                                                   'positive_len',\n",
       "                                                   'negative_len',\n",
       "                                                   'position_len']),\n",
       "                                                 ('ca...\n",
       "                 StackingClassifier(estimators=[('svc',\n",
       "                                                 SVC(C=50, degree=1,\n",
       "                                                     gamma='auto',\n",
       "                                                     probability=True)),\n",
       "                                                ('mlp',\n",
       "                                                 MLPClassifier(alpha=0.1,\n",
       "                                                               hidden_layer_sizes=(10,\n",
       "                                                                                   10,\n",
       "                                                                                   10),\n",
       "                                                               max_iter=2000,\n",
       "                                                               random_state=1000)),\n",
       "                                                ('rfc',\n",
       "                                                 RandomForestClassifier(max_depth=10,\n",
       "                                                                        min_samples_leaf=0.005,\n",
       "                                                                        min_samples_split=0.005,\n",
       "                                                                        n_estimators=500,\n",
       "                                                                        n_jobs=-1,\n",
       "                                                                        random_state=1000))],\n",
       "                                    final_estimator=LogisticRegression(),\n",
       "                                    n_jobs=-1))])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e5db80a6-af5e-48e5-982a-77cbe7907262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the position key words\n",
    "# (\n",
    "#     tfidf_pipe\n",
    "#     .named_steps['transformer']\n",
    "#     .transformers_[2][1]\n",
    "#     .named_steps['transformer']\n",
    "#     .transformers_[0][1]\n",
    "#     .get_feature_names()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "89bf3fc4-3359-48d9-b4d4-09e4f1a829ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipe(pipe):\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    print('f1_score:', f1_score(y_test, y_pred, average='weighted'))\n",
    "    print('accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "850bd5c1-b259-4077-9fa5-e66ad68bf464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumbit(pipe):\n",
    "    sub_df = pd.read_csv('data/HeadHunter_sample_submit.csv')\n",
    "    submittions = pipe.predict(test_df)\n",
    "    sub_df['target'] = submittions\n",
    "    return sub_df.to_csv('submittion.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cc22a0d8-e63d-4ec2-9750-e62a4d009360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.7611512647683535\n",
      "accuracy: 0.7887262805407417\n"
     ]
    }
   ],
   "source": [
    "test_pipe(tfidf_pipe)  # 0.7587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db58209d-592f-4095-94b5-83d59e404645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumbit(tfidf_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f1e95e9-57bf-413f-a53a-5be7ca2e829b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yb/ysqh2wxs6v1_4q_92fnn6q380000gn/T/ipykernel_44441/2208162487.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models/tfidf_pipe_0.7335.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "# joblib.dump(tfidf_pipe, 'models/tfidf_pipe_0.7335.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a69fd47-e9a8-402c-90cc-c628b0118799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63f0cdc9-a036-498f-8038-1558cf7f777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a473b25d-fe1f-442e-93fb-c33602bfe177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c2d2228-609b-4aff-8631-d2b6be6f0d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HHWord2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.emb_size = model.vector_size\n",
    "    \n",
    "    def text2vec(self, text):\n",
    "        vector = np.array([.0 for _ in range(self.emb_size)])\n",
    "        count = 0\n",
    "\n",
    "        for word in text.split():\n",
    "            if word in self.model.wv:\n",
    "                vector += self.model.wv[word]\n",
    "                count += 1\n",
    "\n",
    "        if count != 0:\n",
    "            vector /= count\n",
    "\n",
    "        return vector\n",
    "\n",
    "    def fit(self,X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None): \n",
    "        vectors = np.zeros((X.shape[0], self.emb_size))\n",
    "        \n",
    "        for i, text in tqdm(enumerate(X), total=len(X)):\n",
    "            vectors[i, :] = self.text2vec(text)\n",
    "            \n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ae9f7f4-7277-4589-b848-62f27ecc79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('hh_word2vec.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b806137-f13b-42ff-8dc2-e5a6c4bdfcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_transformer = get_text_transformer(HHWord2VecTransformer(model))\n",
    "word2vec_pipe = pipe_with_text_transformer(word2vec_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23966c95-e34d-4da8-bb1a-54f93242b379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 37627/37627 [00:04<00:00, 8445.65it/s]\n",
      "100%|██████████████████████████████████| 37627/37627 [00:03<00:00, 10438.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 5s, sys: 1.17 s, total: 2min 6s\n",
      "Wall time: 2min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transformer',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['salary_rating',\n",
       "                                                   'team_rating',\n",
       "                                                   'managment_rating',\n",
       "                                                   'career_rating',\n",
       "                                                   'workplace_rating',\n",
       "                                                   'rest_recovery_rating',\n",
       "                                                   'positive_len',\n",
       "                                                   'negative_len',\n",
       "                                                   'position_len']),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('imputer',...\n",
       "                                                                                                   ('text_normal_positive',\n",
       "                                                                                                    HHWord2VecTransformer(model=<gensim.models.word2vec.Word2Vec object at 0x133d7a0a0>),\n",
       "                                                                                                    'normal_positive'),\n",
       "                                                                                                   ('text_normal_negative',\n",
       "                                                                                                    HHWord2VecTransformer(model=<gensim.models.word2vec.Word2Vec object at 0x133d7a0a0>),\n",
       "                                                                                                    'normal_negative')]))]),\n",
       "                                                  ['normal_positive',\n",
       "                                                   'normal_negative',\n",
       "                                                   'normal_position'])])),\n",
       "                ('model', RandomForestClassifier())])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "word2vec_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2baf138-1ef5-49e2-a4e4-1616960cee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 16126/16126 [00:01<00:00, 8495.76it/s]\n",
      "100%|██████████████████████████████████| 16126/16126 [00:01<00:00, 10637.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.7286120187241485\n",
      "accuracy_score: 0.7499689941709041\n"
     ]
    }
   ],
   "source": [
    "y_pred = word2vec_pipe.predict(X_test)\n",
    "\n",
    "print('f1_score:', f1_score(y_test, y_pred, average='weighted')) # 0.729\n",
    "print('accuracy_score:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "05e74401-c74f-48c6-a1b6-2d28a1eb2331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 50651/50651 [00:04<00:00, 11125.47it/s]\n",
      "100%|██████████████████████████████████| 50651/50651 [00:04<00:00, 12274.51it/s]\n"
     ]
    }
   ],
   "source": [
    "sumbit(word2vec_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "ebbb7801-5f20-4726-8405-784693d4ef4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/word2vec_pipe_0.729.pickle']"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(word2vec_pipe, 'models/word2vec_pipe_0.729.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
